{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Notes following [Sentdex's Neural Networks from Scratch in Python video series](https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3) in an attempt to properly learn/reinforce the basics of ML and neural net structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Intro\n",
    "\n",
    "## A Singular Neuron (Node)\n",
    "[Video](https://www.youtube.com/watch?v=Wo5dMEP_BbI)\n",
    "\n",
    "This is a simple demonstration of a neuron.\n",
    "\n",
    "### Context\n",
    "A neural network is made up of many layers, each of which is made up of neurons.\n",
    "\n",
    "### Neuron Components\n",
    "- 3 Inputs (From previous layer of neurons)\n",
    "- 3 Weights (From synapses connecting to prevoius layers; modifies inputs)\n",
    "- Bias (A singular value added to the neuron's output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.839999999999996\n"
     ]
    }
   ],
   "source": [
    "inputs = [4.4, 3.2, 2.8]\n",
    "weights = [5.8, 2.7, 3.1]\n",
    "bias = 2\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Layer of Neurons\n",
    "[Video](https://www.youtube.com/watch?v=lGLto9Xd7bU)\n",
    "\n",
    "This is a rudimentary demonstration of an output layer. \n",
    "\n",
    "### Components\n",
    "- 3 neurons, each of which has 4 inputs and 1 output.\n",
    "\n",
    "### More Info\n",
    "- The inputs are  the same for all 3 nodes, as each node would theoretically take inputs from all nodes of the previous layer.\n",
    "- Since this is a layer of 3 neurons, the output consists of 3 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.89999999999999, 43.86, -6.799999999999999]\n"
     ]
    }
   ],
   "source": [
    "inputs = [4.4, 3.2, -2.8, 5.1]\n",
    "\n",
    "node1_weights = [5.8, 2.7, 3.1, 4.2]\n",
    "node1_bias = 2\n",
    "\n",
    "node2_weights = [-0.3, 2.3, -3.0, 4.2]\n",
    "node2_bias = 8\n",
    "\n",
    "node3_weights = [0.1, 6.1, 4.8, -3.2]\n",
    "node3_bias = 3\n",
    "\n",
    "layer_outputs = [\n",
    "    inputs[0]*node1_weights[0] + inputs[1]*node1_weights[1] + inputs[2]*node1_weights[2] + inputs[3]*node1_weights[3] + node1_bias,\n",
    "    inputs[0]*node2_weights[0] + inputs[1]*node2_weights[1] + inputs[2]*node2_weights[2] + inputs[3]*node2_weights[3] + node2_bias,\n",
    "    inputs[0]*node3_weights[0] + inputs[1]*node3_weights[1] + inputs[2]*node3_weights[2] + inputs[3]*node3_weights[3] + node3_bias\n",
    "]\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Reimplement the layer code above by grouping weights/biases into arrays and calculating output by interating through them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.89999999999999, 43.86, -6.799999999999999]\n"
     ]
    }
   ],
   "source": [
    "inputs = [4.4, 3.2, -2.8, 5.1]            # same for all nodes\n",
    "\n",
    "layer_weights = [[5.8, 2.7, 3.1, 4.2],    # node 1\n",
    "                 [-0.3, 2.3, -3.0, 4.2],  # node 2\n",
    "                 [0.1, 6.1, 4.8, -3.2]]   # node 3\n",
    "\n",
    "biases = [2, 8, 3]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "# combine/zip weights & biases into a single list of tuples, which we read from\n",
    "for neuron_weights, neuron_bias in zip(layer_weights, biases):\n",
    "    neuron_output = 0\n",
    "    for neuron_input, neuron_weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += neuron_input*neuron_weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "    \n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Intro\n",
    "[Video](https://www.youtube.com/watch?v=TEWy9vZcxW4)\n",
    "\n",
    "### DEFINITION: Shape \n",
    "The size of the array in each dimension.\n",
    "\n",
    "| Array                          | Shape    | Type             |\n",
    "|:-------------------------------|:---------|:-----------------|\n",
    "| `[1, 2, 3, 4]`                 | `(4)`    | 1D Array, Vector |\n",
    "| `[[1, 2, 3, 4], [2, 5, 3, 1]]` | `(4, 2)` | 2D Array, Matrix |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Use vector dot products to further simply the process from the Neuron demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 3, 5, 2]\n",
    "weights = [0.2, 0.8, 0.6, -0.1]\n",
    "bias = 1.8\n",
    "\n",
    "# Dot product order doesn't matter since inputs & weight are the same size\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Use vector dot products to further simply the process from the Layers demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.8  8.  39. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 3, 5, 2]\n",
    "layer_weights = [[5.8, 2.7, 3.1, 4.2],\n",
    "                 [-0.3, 2.3, -3.0, 4.2], \n",
    "                 [0.1, 6.1, 4.8, -3.2]]\n",
    "biases = [2, 8, 3]\n",
    "\n",
    "# Dot product order of layer weights & inputs matters because the two have difference dimensions!\n",
    "# In a typical machine learning library, incorrect order would result in a shape error.\n",
    "output = np.dot(layer_weights, inputs) + biases\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches/Layers/Objects\n",
    "[Video](https://www.youtube.com/watch?v=TEWy9vZcxW4)\n",
    "\n",
    "### DEFINITION: Batch\n",
    "A batch is a collection of inputs (samples). Batching data allows us to show the network multiple samples at a time, allowing it to generalize more effectively.\n",
    "\n",
    "- When batch size is **too small**, it takes a long time to parse the entire dataset.\n",
    "- When batch size is **too big**, the model's ability to generalize is severely impacted.\n",
    "- Typical batch size is between 32 and 64.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Modify the previous example with batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39.8   13.29 -26.7 ]\n",
      " [  2.    25.21 -29.1 ]\n",
      " [ 38.    -9.93  82.5 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [[1, 3, 5, 2],\n",
    "          [2, 0.7, -4, 1],\n",
    "          [-3, 6, 3, -9]]\n",
    "\n",
    "# Weights & Biases don't change, as the neuron layer stays the same\n",
    "layer_weights = [[5.8, 2.7, 3.1, 4.2],\n",
    "                 [-0.3, 2.3, -3.0, 4.2], \n",
    "                 [0.1, 6.1, 4.8, -3.2]]\n",
    "biases = [2, 8, 3]\n",
    "\n",
    "# We can't take the dot product of two 3x4 matrices, so we need to tranpose the inputs for the dot product to work.\n",
    "# The end result for the calculation is the same as multiplying every input by every weight.\n",
    "output = np.dot(layer_weights, np.array(inputs).transpose()) + biases\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPT/EXAMPLE: Multi-Layer Network\n",
    "Neural networks are made up of multiple layers, consisting of an *input layer* (X), a series of *hidden layers*, and an *output layer*.\n",
    "\n",
    "### More Info\n",
    "In this new neural network:\n",
    "- Weights are initialized randomly between -1 and 1. Keeping these values small means that we're not making the data values bigger and bigger as they pass through the neural network. This saves memory and consequently prevents stack overflow errors.\n",
    "- We should avoid weights of 0, as they can result in outputs with values of 0 and cause a \"dead network\". Hint: Initialize biases as non-zero values.\n",
    "- This network is made up of *dense layers*, or fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36077223  0.34039262]\n",
      " [ 0.16444803 -0.07122948]\n",
      " [ 0.34158758 -0.02024933]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Input data consists of 3 samples.\n",
    "X = [[1, 3, 5, 2],\n",
    "     [2, 0.7, -4, 1],\n",
    "     [-3, 6, 3, -9]]\n",
    "\n",
    "class DenseLayer:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        # Randomly generate 2D array of weights (rows, columns). \n",
    "        # The dimensions are already flipped, removing the need to transpose.\n",
    "        self.weights = 0.10 * np.random.randn(num_inputs, num_neurons)\n",
    "        # Biases are a vector of zeroes, `num_neurons` in length\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "layer1 = DenseLayer(4, 5)\n",
    "layer2 = DenseLayer(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "# layer 1's output becomes layer 2's input\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "[Video](https://www.youtube.com/watch?v=gmjzbpSVY1A)\n",
    "\n",
    "An activation function defines the neural network's output, which has to be simple enough to actually act upon. For example, outputting an image rating from 0 to 1 is simpler than outputting said image rating as a vector or 2D array.\n",
    "\n",
    "\n",
    "### Unit Step Function\n",
    "The unit step function is a step function where values x-below 0 will result in an output of 0 and x-values of above 0 will result in an output of 1. This allows complex neural networks to output very simple binary data.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/1280px-Dirac_distribution_CDF.svg.png\" width=400>\n",
    "\n",
    "### Sigmoid Funtion\n",
    "Using a sigmoid function allows for more granular output than the unit step function, as it outputs a decimal that can be anywhere between -1 and 1, depending on X.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\" width=400>\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "When input is less than 0, output is 0, and when the input is greater than 0, the output is directly proportional to the input. Here, the output can still be granular, but unlike with the sigmoid function, it can't be less than 0.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/faq/relu-derivative/relu_3.png\" width=400>\n",
    "\n",
    "#### Why use ReLU over Sigmoid?\n",
    "- **It's fast.** Since it's linear, it's much faster to calculate than Sigmoid.\n",
    "- **It's simple.** Being linear and not allowing for values less than 0 makes ReLU easier to incorporate into optimization algorithms.\n",
    "\n",
    "#### Why not use *y=x* as an activation function instead?\n",
    "Since it includes all values below 0 as well, it can only approximate linear data (or rather, it approximates all data with a line).\n",
    "\n",
    "#### Features & Usage\n",
    "- Changing the weights (multiplication) changes the slope.\n",
    "- Changing the biases (addition) changes the activation point, translating the graph left/right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Implement a simple ReLU example. The fundamental result is that everything 0-and-below gets clipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 3, 6, 2, 0, 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0, 1, -5, 3, 6, 2, -1, 7]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "#   NOTE: SAME RESULT AS...\n",
    "#   if i > 0:\n",
    "#       output.append(i)\n",
    "#   else:\n",
    "#       output.append(0)\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "Use the ReLU activation function to modify the output of a neuron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 1 OUTPUT\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 7.69311704e-04  7.84235811e-04 -2.28373517e-03 -9.87815185e-04\n",
      "  -3.27828448e-04]\n",
      " [ 1.77620579e-04  2.83317359e-03 -4.52324829e-03 -1.80507772e-03\n",
      "  -1.01027115e-03]\n",
      " ...\n",
      " [-1.27651001e-01  1.87989044e-01 -1.00373679e-01 -2.52528307e-02\n",
      "  -5.77054642e-02]\n",
      " [-1.73879868e-01  1.20034589e-01  6.82417861e-02  4.64914810e-02\n",
      "  -3.06659630e-02]\n",
      " [-1.34910633e-01  1.90738547e-01 -9.41162688e-02 -2.19667053e-02\n",
      "  -5.81886685e-02]]\n",
      "ACTIVATION FUNCTION OUTPUT\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [7.69311704e-04 7.84235811e-04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.77620579e-04 2.83317359e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 1.87989044e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.20034589e-01 6.82417861e-02 4.64914810e-02\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.90738547e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "# Sentdex's custom dataset.\n",
    "# Based on # https://cs231n.github.io/neural-networks-case-study/\n",
    "from nnfs.datasets import spiral_data \n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# X denotes inputs, y denotes targets/classification\n",
    "X, y = spiral_data(100, 3) # 100 sets of 3 classes each\n",
    "\n",
    "class DenseLayer:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        # Randomly generate 2D array of weights (rows, columns)\n",
    "        self.weights = 0.10 * np.random.randn(num_inputs, num_neurons)\n",
    "        # Biases are a vector of zeroes, `num_neurons` in length\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs) # clip values of 0 and below\n",
    "\n",
    "layer1 = DenseLayer(2, 5)\n",
    "activation1 = ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output) # transform output with activation function\n",
    "\n",
    "print(\"LAYER 1 OUTPUT\\n{}\".format(layer1.output))\n",
    "print(\"ACTIVATION FUNCTION OUTPUT\\n{}\".format(activation1.output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
